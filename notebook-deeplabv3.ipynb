{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7527747,"sourceType":"datasetVersion","datasetId":4384568}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#1. Setup and Imports:\n\nImport necessary libraries.","metadata":{"id":"_KYjQ5QQ-nbE"}},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"A7uo0yEs-tJD","executionInfo":{"status":"ok","timestamp":1706728523369,"user_tz":-60,"elapsed":6662,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:44:06.743106Z","iopub.execute_input":"2024-02-01T13:44:06.743533Z","iopub.status.idle":"2024-02-01T13:44:06.750466Z","shell.execute_reply.started":"2024-02-01T13:44:06.743504Z","shell.execute_reply":"2024-02-01T13:44:06.748905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Preprocessing\nDefine functions to load and preprocess data\n","metadata":{"id":"zvTKSiCd-z4Z"}},{"cell_type":"code","source":"from tifffile import TiffFile\nimport numpy as np\nimport torch\nfrom torchvision import transforms\n\ndef load_image(image_path):\n    with TiffFile(image_path) as tif:\n        image = tif.asarray()\n    # Normalize the image to [0, 1]\n    image = image.astype(np.float32) / 65535.0\n    return image\n\ndef load_mask(mask_path):\n    with TiffFile(mask_path) as tif:\n        mask = tif.asarray()\n    # Directly return the mask as it is (assuming it's already in an appropriate range 0-9)\n    return mask\n","metadata":{"id":"LVp3qHx94vZC","executionInfo":{"status":"ok","timestamp":1706728545832,"user_tz":-60,"elapsed":2083,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:44:06.753857Z","iopub.execute_input":"2024-02-01T13:44:06.754768Z","iopub.status.idle":"2024-02-01T13:44:06.766154Z","shell.execute_reply.started":"2024-02-01T13:44:06.754706Z","shell.execute_reply":"2024-02-01T13:44:06.764995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exemple usage","metadata":{"id":"v2cpjawT_BYf"}},{"cell_type":"markdown","source":"# 3. Dataset and DataLoader\n\n\n*   Create a custom PyTorch Dataset class for your Sentinel-2 data.\n\n*   Use PyTorch DataLoader to batch and shuffle the data.","metadata":{"id":"Kw7x9zdG_IgI"}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n","metadata":{"id":"FldUWGAL96mC","executionInfo":{"status":"ok","timestamp":1706728545834,"user_tz":-60,"elapsed":15,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:44:06.767465Z","iopub.execute_input":"2024-02-01T13:44:06.768942Z","iopub.status.idle":"2024-02-01T13:44:06.787272Z","shell.execute_reply.started":"2024-02-01T13:44:06.768893Z","shell.execute_reply":"2024-02-01T13:44:06.785830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Sentinel2Dataset(Dataset):\n    def __init__(self, images_dir, masks_dir, transform=None):\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.transform = transform\n        self.images = os.listdir(images_dir)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.images_dir, self.images[idx])\n        mask_name = os.path.join(self.masks_dir, self.images[idx])\n\n        image = load_image(img_name)  # Normalized image\n        mask = load_mask(mask_name)   # Categorical mask\n\n        # Convert to PyTorch tensors\n        image_tensor = torch.from_numpy(image).float().permute(2, 0, 1)\n        mask_tensor = torch.from_numpy(mask).long()  # Convert mask to long tensor\n\n        return image_tensor, mask_tensor","metadata":{"id":"_35GshXg_Osq","executionInfo":{"status":"ok","timestamp":1706728545835,"user_tz":-60,"elapsed":13,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:44:06.790391Z","iopub.execute_input":"2024-02-01T13:44:06.791718Z","iopub.status.idle":"2024-02-01T13:44:06.803596Z","shell.execute_reply.started":"2024-02-01T13:44:06.791668Z","shell.execute_reply":"2024-02-01T13:44:06.802476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define directories\nimages_dir = '/kaggle/input/train/images/'\nmasks_dir = '/kaggle/input/train/masks/'\n\n# Instantiate the dataset\nsentinel_dataset = Sentinel2Dataset(images_dir=images_dir, masks_dir=masks_dir)\n\n# Define batch size\nbatch_size = 16\n\n# Create DataLoader\ndata_loader = DataLoader(sentinel_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","metadata":{"id":"TEjr4w-SB6hR","executionInfo":{"status":"ok","timestamp":1706728658691,"user_tz":-60,"elapsed":112868,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:44:06.805409Z","iopub.execute_input":"2024-02-01T13:44:06.806163Z","iopub.status.idle":"2024-02-01T13:44:06.832831Z","shell.execute_reply.started":"2024-02-01T13:44:06.806120Z","shell.execute_reply":"2024-02-01T13:44:06.830907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define validation directories\nval_images_dir = '/kaggle/input/test/images/'\nval_masks_dir = '/kaggle/input/test/masks/'\n\n# Instantiate the validation dataset\nval_dataset = Sentinel2Dataset(images_dir=val_images_dir, masks_dir=val_masks_dir)\n\n# Create validation DataLoader\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","metadata":{"id":"SWSVh5F4zsyU","executionInfo":{"status":"ok","timestamp":1706728660240,"user_tz":-60,"elapsed":1555,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:44:06.836466Z","iopub.execute_input":"2024-02-01T13:44:06.837341Z","iopub.status.idle":"2024-02-01T13:44:06.848218Z","shell.execute_reply.started":"2024-02-01T13:44:06.837303Z","shell.execute_reply":"2024-02-01T13:44:06.846851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#4. Model Architecture (U-Net):\n\n* Define the U-Net architecture.\n* Modify the final layer to output 10 channels (one for each category).","metadata":{"id":"DJDNZtkgD5pS"}},{"cell_type":"code","source":"import torchvision.models.segmentation as segmentation\nimport torch.nn as nn\nimport torch\n\ndef create_deeplabv3plus(num_classes, in_channels=4):\n    # Load pre-trained DeepLabV3+ with ResNet backbone\n    model = segmentation.deeplabv3_resnet50(num_classes=num_classes)\n\n    # Modify the first convolution layer to accept 4-channel input\n    old_conv = model.backbone.conv1\n    new_conv = nn.Conv2d(in_channels, old_conv.out_channels, kernel_size=old_conv.kernel_size, \n                         stride=old_conv.stride, padding=old_conv.padding, bias=old_conv.bias)\n\n    # Copy the weights from the old conv layer\n    old_weights = old_conv.weight.detach()\n    new_weights = torch.zeros_like(new_conv.weight)\n    new_weights[:, :3, :, :] = old_weights\n\n    # Initialize the weights for the NIR channel by copying the red channel's weights\n    new_weights[:, 3, :, :] = old_weights[:, 0, :, :]\n\n    new_conv.weight = nn.Parameter(new_weights)\n\n    # Replace the first convolutional layer\n    model.backbone.conv1 = new_conv\n\n    return model\n\n# Example usage\nnum_classes = 10\nmodel = create_deeplabv3plus(num_classes, in_channels=4)\n","metadata":{"id":"VrMQShmdESEM","executionInfo":{"status":"ok","timestamp":1706728660241,"user_tz":-60,"elapsed":9,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:44:06.850473Z","iopub.execute_input":"2024-02-01T13:44:06.851183Z","iopub.status.idle":"2024-02-01T13:44:07.774060Z","shell.execute_reply.started":"2024-02-01T13:44:06.851147Z","shell.execute_reply":"2024-02-01T13:44:07.772692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explanation\n\n* DoubleConv: A helper class to perform two consecutive sets of convolution,batch normalization, and ReLU operations.\n* UNet: The main U-Net architecture, which includes:\n  * Initial Convolution (inc): The first layer of the U-Net, increasing the number of channels.\n  * Downsampling Path (down1, down2, down3, down4): Each step involves a DoubleConv operation.\n  * Upsampling Path (up1, up2, up3, up4): Each step involves a DoubleConv operation with concatenated feature maps from the corresponding downsampling layer and the previous upsampling layer.\n  * Final Convolution (outc): A 1x1 convolution to map the final feature maps to the number of classes.","metadata":{"id":"68VQ4Wn0EWjW"}},{"cell_type":"code","source":"# Check if GPU is available and move the model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel= nn.DataParallel(model)\nmodel.to(device)\n\n","metadata":{"id":"SrsuxKtgIPA9","executionInfo":{"status":"ok","timestamp":1706728661189,"user_tz":-60,"elapsed":526,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:44:07.776884Z","iopub.execute_input":"2024-02-01T13:44:07.777278Z","iopub.status.idle":"2024-02-01T13:44:07.794628Z","shell.execute_reply.started":"2024-02-01T13:44:07.777246Z","shell.execute_reply":"2024-02-01T13:44:07.793349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)\n","metadata":{"id":"seEjJV4YIQ8B","executionInfo":{"status":"ok","timestamp":1706728661190,"user_tz":-60,"elapsed":20,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"outputId":"719ad109-e49a-4cab-8cce-56589cbf756d","execution":{"iopub.status.busy":"2024-02-01T13:44:07.796238Z","iopub.execute_input":"2024-02-01T13:44:07.797116Z","iopub.status.idle":"2024-02-01T13:44:07.806574Z","shell.execute_reply.started":"2024-02-01T13:44:07.797062Z","shell.execute_reply":"2024-02-01T13:44:07.805193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Function and Optimizer:\n\n* Implement or use an existing Dice Loss function.\n* Choose an optimizer (e.g., Adam).","metadata":{"id":"TkUnhoDwLg43"}},{"cell_type":"code","source":"import torch\nfrom torch import Tensor\n\n\ndef dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n    # Average of Dice coefficient for all batches, or for a single mask\n    assert input.size() == target.size()\n    assert input.dim() == 3 or not reduce_batch_first\n\n    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n\n    inter = 2 * (input * target).sum(dim=sum_dim)\n    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n\n    dice = (inter + epsilon) / (sets_sum + epsilon)\n    return dice.mean()\n\n\ndef multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n    # Average of Dice coefficient for all classes\n    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)\n\n\ndef dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n    # Dice loss (objective to minimize) between 0 and 1\n    fn = multiclass_dice_coeff if multiclass else dice_coeff\n    return 1 - fn(input, target, reduce_batch_first=True)","metadata":{"id":"YsXOmft-wrM1","executionInfo":{"status":"ok","timestamp":1706728661190,"user_tz":-60,"elapsed":16,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:44:07.808438Z","iopub.execute_input":"2024-02-01T13:44:07.809253Z","iopub.status.idle":"2024-02-01T13:44:07.821123Z","shell.execute_reply.started":"2024-02-01T13:44:07.809217Z","shell.execute_reply":"2024-02-01T13:44:07.819893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"MYKkLpGawupH","executionInfo":{"status":"ok","timestamp":1706728661190,"user_tz":-60,"elapsed":15,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlearning_rate = 1e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n#\ncriterion = nn.CrossEntropyLoss()\n","metadata":{"id":"vv09fvsQNvU0","executionInfo":{"status":"ok","timestamp":1706728661191,"user_tz":-60,"elapsed":14,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:44:07.822620Z","iopub.execute_input":"2024-02-01T13:44:07.823307Z","iopub.status.idle":"2024-02-01T13:44:07.839749Z","shell.execute_reply.started":"2024-02-01T13:44:07.823275Z","shell.execute_reply":"2024-02-01T13:44:07.838072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch import optim\nimport torch.utils.data as data\nimport torchvision.utils as utils\nfrom pathlib import Path\nimport tifffile\nimport numpy as np\n\n","metadata":{"id":"xAwN5vgd4--b","executionInfo":{"status":"ok","timestamp":1706728661191,"user_tz":-60,"elapsed":13,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:44:07.841470Z","iopub.execute_input":"2024-02-01T13:44:07.842157Z","iopub.status.idle":"2024-02-01T13:44:07.851556Z","shell.execute_reply.started":"2024-02-01T13:44:07.842121Z","shell.execute_reply":"2024-02-01T13:44:07.850501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef save_input_as_tiff(input_tensor, filename):\n    # Revert the normalization (assuming original range was 0-65535)\n    array = (input_tensor.detach().cpu().numpy() * 65535).astype(np.uint16)\n\n    # If the tensor is a 3D tensor (C, H, W), convert it to (H, W, C)\n    if array.ndim == 3:\n        array = np.transpose(array, (1, 2, 0))\n\n    # Save as TIFF in original format\n    tifffile.imwrite(filename, array)\n\ndef save_mask_as_tiff(mask_tensor, filename):\n    # Convert to NumPy array\n    array = mask_tensor.detach().cpu().numpy()\n\n    # No need to transpose masks as they should be [H, W]\n    tifffile.imwrite(filename, array)\n","metadata":{"id":"T3TYG7R1XqGE","executionInfo":{"status":"ok","timestamp":1706728661191,"user_tz":-60,"elapsed":13,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:44:07.852867Z","iopub.execute_input":"2024-02-01T13:44:07.853897Z","iopub.status.idle":"2024-02-01T13:44:07.864534Z","shell.execute_reply.started":"2024-02-01T13:44:07.853862Z","shell.execute_reply":"2024-02-01T13:44:07.863576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"      # Create a directory if it is not there, so we can save files and results in it\n      from pathlib import Path\n      Path('/kaggle/working/result/predicted').mkdir(parents=True, exist_ok=True)  \n      Path('/kaggle/working/result/original').mkdir(parents=True, exist_ok=True)\n      Path('/kaggle/working/result/label').mkdir(parents=True, exist_ok=True)\n    \n      Path('/kaggle/working/val/predicted').mkdir(parents=True, exist_ok=True)  \n      Path('/kaggle/working/val/original').mkdir(parents=True, exist_ok=True)\n      Path('/kaggle/working/val/label').mkdir(parents=True, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T13:44:07.866060Z","iopub.execute_input":"2024-02-01T13:44:07.866454Z","iopub.status.idle":"2024-02-01T13:44:07.882496Z","shell.execute_reply.started":"2024-02-01T13:44:07.866425Z","shell.execute_reply":"2024-02-01T13:44:07.881346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n\nnum_classes = 10\ncolumns = ['Epoch', 'Train Loss', 'Validation Loss', 'Validation Dice'] + [f'Class {i} {metric}' for i in range(num_classes) for metric in ['Sensitivity', 'Specificity', 'Precision', 'IOU']]\nmetrics_data = pd.DataFrame(columns=columns)\n\n\nfor epoch in range(num_epochs):\n  model.train()\n  total_train_loss = 0.0\n\n  # Set up tqdm progress bar\n  progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n  for batch_idx, (inputs, targets) in progress_bar:\n      inputs = inputs.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n      targets = targets.to(device=device, dtype=torch.long)\n\n      model_output = model(inputs)\n      outputs = model_output['out']  # Extracting the main output\n\n        # Loss computation\n      loss = criterion(outputs, targets)\n      loss += dice_loss(\n            F.softmax(outputs, dim=1).float(),\n            F.one_hot(targets, num_classes).permute(0, 3, 1, 2).float(),\n            multiclass=True\n        )\n      # Backward and optimize\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      total_train_loss += loss.item()\n      progress_bar.set_postfix({\n            'loss': f'{total_train_loss / (batch_idx + 1):.4f}'\n        })\n      if batch_idx % 100 == 0 or batch_idx == len(data_loader) - 1:\n          save_input_as_tiff(inputs[0], f\"/kaggle/working/result/original/original_image_{batch_idx}_{epoch}.tif\")\n          save_mask_as_tiff(targets[0], f\"/kaggle/working/result/label/label_image_{batch_idx}_{epoch}.tif\")\n          predicted_mask = torch.argmax(outputs[0],dim=0)\n          save_mask_as_tiff(predicted_mask,f\"/kaggle/working/result/predicted/predicted_image_{batch_idx}_{epoch}.tif\")\n          torch.save(model.module.state_dict(),Path(f'/kaggle/working/result/unet_{batch_idx}_{epoch}.pkl'))\n\n            \n    # Validation Loop\n    \n  sensitivities, specificities, precisions, ious = [[] for _ in range(num_classes)], [[] for _ in range(num_classes)], [[] for _ in range(num_classes)], [[] for _ in range(num_classes)]\n    \n  model.eval()\n  total_val_loss = 0.0\n  total_val_dice = 0\n  _ = 0\n  with torch.no_grad():\n      for inputs, targets in val_loader:\n          _ += 1\n          inputs, targets = inputs.to(device), targets.to(device)\n          outputs = model(inputs)\n          loss = criterion(outputs, targets)\n          total_val_loss += loss.item()\n                    # Convert outputs to probabilities\n          probabilities = torch.softmax(outputs, dim=1)\n\n          # Convert targets to one-hot format\n          target_one_hot = F.one_hot(targets, num_classes=num_classes).permute(0, 3, 1, 2).float()\n\n          # Calculate Dice coefficient\n          dice_score = multiclass_dice_coeff(probabilities, target_one_hot)\n          total_val_dice += dice_score\n          if _ % 100 == 0 or _ == len(val_loader) - 1:\n              save_input_as_tiff(inputs[0], f\"/kaggle/working/val/original/original_image_{batch_idx}_{epoch}.tif\")\n              save_mask_as_tiff(targets[0], f\"/kaggle/working/val/label/label_image_{batch_idx}_{epoch}.tif\")\n              predicted_mask = torch.argmax(outputs[0],dim=0)\n              save_mask_as_tiff(predicted_mask,f\"/kaggle/working/val/predicted/predicted_image_{batch_idx}_{epoch}.tif\")\n\n                  # Calculate additional metrics\n          preds = torch.argmax(outputs, dim=1)\n          for class_idx in range(num_classes):\n            true_positive = (preds == class_idx) & (targets == class_idx)\n            true_negative = (preds != class_idx) & (targets != class_idx)\n            false_positive = (preds == class_idx) & (targets != class_idx)\n            false_negative = (preds != class_idx) & (targets == class_idx)\n\n            TP = true_positive.sum().item()\n            TN = true_negative.sum().item()\n            FP = false_positive.sum().item()\n            FN = false_negative.sum().item()\n\n            sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n            specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n            precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n            iou = TP / (TP + FP + FN) if (TP + FP + FN) != 0 else 0\n\n            sensitivities[class_idx].append(sensitivity)\n            specificities[class_idx].append(specificity)\n            precisions[class_idx].append(precision)\n            ious[class_idx].append(iou)\n\n  #Calculate average metrics\n  avg_train_loss = total_train_loss / len(data_loader)\n    \n  avg_val_loss = total_val_loss / len(val_loader)\n  avg_val_dice = total_val_dice / len(val_loader)\n    \n  avg_metrics = [epoch + 1, avg_train_loss, avg_val_loss, avg_val_dice]\n  for class_idx in range(num_classes):\n    avg_sensitivity = sum(sensitivities[class_idx]) / len(sensitivities[class_idx])\n    avg_specificity = sum(specificities[class_idx]) / len(specificities[class_idx])\n    avg_precision = sum(precisions[class_idx]) / len(precisions[class_idx])\n    avg_iou = sum(ious[class_idx]) / len(ious[class_idx])\n    avg_metrics.extend([avg_sensitivity, avg_specificity, avg_precision, avg_iou])\n\n  metrics_data = metrics_data.append(pd.Series(avg_metrics, index=columns), ignore_index=True)\n  metrics_data.to_csv('/kaggle/working/result/metrics_data.csv', index=False)\n","metadata":{"id":"WhklIV8xOsgE","outputId":"b1e73f7a-3578-4920-e8a5-46d5a5ddd164","execution":{"iopub.status.busy":"2024-02-01T13:52:41.686777Z","iopub.execute_input":"2024-02-01T13:52:41.687257Z","iopub.status.idle":"2024-02-01T13:53:39.898941Z","shell.execute_reply.started":"2024-02-01T13:52:41.687219Z","shell.execute_reply":"2024-02-01T13:53:39.896573Z"},"trusted":true},"execution_count":null,"outputs":[]}]}