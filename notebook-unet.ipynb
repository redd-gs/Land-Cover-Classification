{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7527747,"sourceType":"datasetVersion","datasetId":4384568}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#1. Setup and Imports:\n\nImport necessary libraries.\nCheck for GPU availability (important for Google Colab).","metadata":{"id":"_KYjQ5QQ-nbE"}},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"A7uo0yEs-tJD","executionInfo":{"status":"ok","timestamp":1706728523369,"user_tz":-60,"elapsed":6662,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:07:42.411915Z","iopub.execute_input":"2024-02-01T13:07:42.412292Z","iopub.status.idle":"2024-02-01T13:07:42.416964Z","shell.execute_reply.started":"2024-02-01T13:07:42.412263Z","shell.execute_reply":"2024-02-01T13:07:42.415934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Preprocessing\nDefine functions to load and preprocess data\n","metadata":{"id":"zvTKSiCd-z4Z"}},{"cell_type":"code","source":"from tifffile import TiffFile\nimport numpy as np\nimport torch\nfrom torchvision import transforms\n\ndef load_image(image_path):\n    with TiffFile(image_path) as tif:\n        image = tif.asarray()\n    # Normalize the image to [0, 1]\n    image = image.astype(np.float32) / 65535.0\n    return image\n\ndef load_mask(mask_path):\n    with TiffFile(mask_path) as tif:\n        mask = tif.asarray()\n    # Directly return the mask as it is (assuming it's already in an appropriate range 0-9)\n    return mask\n","metadata":{"id":"LVp3qHx94vZC","executionInfo":{"status":"ok","timestamp":1706728545832,"user_tz":-60,"elapsed":2083,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:07:46.580580Z","iopub.execute_input":"2024-02-01T13:07:46.581150Z","iopub.status.idle":"2024-02-01T13:07:51.169387Z","shell.execute_reply.started":"2024-02-01T13:07:46.581111Z","shell.execute_reply":"2024-02-01T13:07:51.168295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exemple usage","metadata":{"id":"v2cpjawT_BYf"}},{"cell_type":"markdown","source":"# 3. Dataset and DataLoader\n\n\n*   Create a custom PyTorch Dataset class for your Sentinel-2 data.\n\n*   Use PyTorch DataLoader to batch and shuffle the data.","metadata":{"id":"Kw7x9zdG_IgI"}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n","metadata":{"id":"FldUWGAL96mC","executionInfo":{"status":"ok","timestamp":1706728545834,"user_tz":-60,"elapsed":15,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:07:55.368163Z","iopub.execute_input":"2024-02-01T13:07:55.368955Z","iopub.status.idle":"2024-02-01T13:07:55.376890Z","shell.execute_reply.started":"2024-02-01T13:07:55.368909Z","shell.execute_reply":"2024-02-01T13:07:55.375980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Sentinel2Dataset(Dataset):\n    def __init__(self, images_dir, masks_dir, transform=None):\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.transform = transform\n        self.images = os.listdir(images_dir)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.images_dir, self.images[idx])\n        mask_name = os.path.join(self.masks_dir, self.images[idx])\n\n        image = load_image(img_name)  # Normalized image\n        mask = load_mask(mask_name)   # Categorical mask\n\n        # Convert to PyTorch tensors\n        image_tensor = torch.from_numpy(image).float().permute(2, 0, 1)\n        mask_tensor = torch.from_numpy(mask).long()  # Convert mask to long tensor\n\n        return image_tensor, mask_tensor","metadata":{"id":"_35GshXg_Osq","executionInfo":{"status":"ok","timestamp":1706728545835,"user_tz":-60,"elapsed":13,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:08:06.604597Z","iopub.execute_input":"2024-02-01T13:08:06.605396Z","iopub.status.idle":"2024-02-01T13:08:06.614804Z","shell.execute_reply.started":"2024-02-01T13:08:06.605356Z","shell.execute_reply":"2024-02-01T13:08:06.613798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define directories\nimages_dir = '/kaggle/input/dataset-tar-gz/train/images/'\nmasks_dir = '/kaggle/input/dataset-tar-gz/train/masks/'\n\n# Instantiate the dataset\nsentinel_dataset = Sentinel2Dataset(images_dir=images_dir, masks_dir=masks_dir)\n\n# Define batch size\nbatch_size = 16\n\n# Create DataLoader\ndata_loader = DataLoader(sentinel_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","metadata":{"id":"TEjr4w-SB6hR","executionInfo":{"status":"ok","timestamp":1706728658691,"user_tz":-60,"elapsed":112868,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T13:08:15.216205Z","iopub.execute_input":"2024-02-01T13:08:15.216555Z","iopub.status.idle":"2024-02-01T13:08:15.651885Z","shell.execute_reply.started":"2024-02-01T13:08:15.216527Z","shell.execute_reply":"2024-02-01T13:08:15.651156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define validation directories\nval_images_dir = '/kaggle/input/dataset-tar-gz/test/images/'\nval_masks_dir = '/kaggle/input/dataset-tar-gz/test/masks/'\n\n# Instantiate the validation dataset\nval_dataset = Sentinel2Dataset(images_dir=val_images_dir, masks_dir=val_masks_dir)\n\n# Create validation DataLoader\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","metadata":{"id":"SWSVh5F4zsyU","executionInfo":{"status":"ok","timestamp":1706728660240,"user_tz":-60,"elapsed":1555,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T07:37:12.002399Z","iopub.execute_input":"2024-02-01T07:37:12.002636Z","iopub.status.idle":"2024-02-01T07:37:12.010430Z","shell.execute_reply.started":"2024-02-01T07:37:12.002615Z","shell.execute_reply":"2024-02-01T07:37:12.009595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#4. Model Architecture (U-Net):\n\n* Define the U-Net architecture.\n* Modify the final layer to output 10 channels (one for each category).","metadata":{"id":"DJDNZtkgD5pS"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef conv_block(in_dim, out_dim, act_fn):\n    model = nn.Sequential(\n        nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_dim),\n        act_fn,)\n    return model\n\n\ndef conv_trans_block(in_dim, out_dim, act_fn):\n    model = nn.Sequential(\n        nn.ConvTranspose2d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n        nn.BatchNorm2d(out_dim),\n        act_fn,)\n    return model\n\n\ndef maxpool():\n    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n    return pool\n\n\ndef conv_block_2(in_dim, out_dim, act_fn):\n    model = nn.Sequential(\n        conv_block(in_dim, out_dim, act_fn),\n        nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_dim),\n    )\n    return model\n\ndef conv_block_3(in_dim, out_dim, act_fn):\n    model = nn.Sequential(\n        conv_block(in_dim, out_dim, act_fn),\n        conv_block(out_dim, out_dim, act_fn),\n        nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_dim),\n    )\n    return model\n\nclass UNet(nn.Module):\n    def __init__(self, in_dim, out_dim, num_filter):\n        super(UNet, self).__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.num_filter = num_filter\n        act_fn = nn.LeakyReLU(0.2, inplace=True)\n\n        self.down_1 = conv_block_2(self.in_dim, self.num_filter, act_fn)\n        self.pool_1 = maxpool()\n        self.down_2 = conv_block_2(self.num_filter * 1, self.num_filter * 2, act_fn)\n        self.pool_2 = maxpool()\n        self.down_3 = conv_block_2(self.num_filter * 2, self.num_filter * 4, act_fn)\n        self.pool_3 = maxpool()\n        self.down_4 = conv_block_2(self.num_filter * 4, self.num_filter * 8, act_fn)\n        self.pool_4 = maxpool()\n\n        self.bridge = conv_block_2(self.num_filter * 8, self.num_filter * 16, act_fn)\n\n        self.trans_1 = conv_trans_block(self.num_filter * 16, self.num_filter * 8, act_fn)\n        self.up_1 = conv_block_2(self.num_filter * 16, self.num_filter * 8, act_fn)\n        self.trans_2 = conv_trans_block(self.num_filter * 8, self.num_filter * 4, act_fn)\n        self.up_2 = conv_block_2(self.num_filter * 8, self.num_filter * 4, act_fn)\n        self.trans_3 = conv_trans_block(self.num_filter * 4, self.num_filter * 2, act_fn)\n        self.up_3 = conv_block_2(self.num_filter * 4, self.num_filter * 2, act_fn)\n        self.trans_4 = conv_trans_block(self.num_filter * 2, self.num_filter * 1, act_fn)\n        self.up_4 = conv_block_2(self.num_filter * 2, self.num_filter * 1, act_fn)\n\n        self.out = nn.Conv2d(self.num_filter, self.out_dim, 3, 1, 1)\n\n\n    def forward(self, input):\n        down_1 = self.down_1(input)\n        pool_1 = self.pool_1(down_1)\n        down_2 = self.down_2(pool_1)\n        pool_2 = self.pool_2(down_2)\n        down_3 = self.down_3(pool_2)\n        pool_3 = self.pool_3(down_3)\n        down_4 = self.down_4(pool_3)\n        pool_4 = self.pool_4(down_4)\n\n        bridge = self.bridge(pool_4)\n\n        trans_1 = self.trans_1(bridge)\n        concat_1 = torch.cat([trans_1, down_4], dim=1)\n        up_1 = self.up_1(concat_1)\n        trans_2 = self.trans_2(up_1)\n        concat_2 = torch.cat([trans_2, down_3], dim=1)\n        up_2 = self.up_2(concat_2)\n        trans_3 = self.trans_3(up_2)\n        concat_3 = torch.cat([trans_3, down_2], dim=1)\n        up_3 = self.up_3(concat_3)\n        trans_4 = self.trans_4(up_3)\n        concat_4 = torch.cat([trans_4, down_1], dim=1)\n        up_4 = self.up_4(concat_4)\n\n        out = self.out(up_4)\n\n        return out","metadata":{"id":"VrMQShmdESEM","executionInfo":{"status":"ok","timestamp":1706728660241,"user_tz":-60,"elapsed":9,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T07:37:12.039779Z","iopub.execute_input":"2024-02-01T07:37:12.040495Z","iopub.status.idle":"2024-02-01T07:37:12.065184Z","shell.execute_reply.started":"2024-02-01T07:37:12.040463Z","shell.execute_reply":"2024-02-01T07:37:12.064246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explanation\n\n* DoubleConv: A helper class to perform two consecutive sets of convolution,batch normalization, and ReLU operations.\n* UNet: The main U-Net architecture, which includes:\n  * Initial Convolution (inc): The first layer of the U-Net, increasing the number of channels.\n  * Downsampling Path (down1, down2, down3, down4): Each step involves a DoubleConv operation.\n  * Upsampling Path (up1, up2, up3, up4): Each step involves a DoubleConv operation with concatenated feature maps from the corresponding downsampling layer and the previous upsampling layer.\n  * Final Convolution (outc): A 1x1 convolution to map the final feature maps to the number of classes.","metadata":{"id":"68VQ4Wn0EWjW"}},{"cell_type":"code","source":"# Model parameters\nin_dim = 4  # For RGB+NIR input\nout_dim = 10  # Number of classes\nnum_filter = 64  # Number of filters in the first layer\n\n# Create the U-Net model instance\nmodel = UNet(in_dim,out_dim,num_filter)","metadata":{"id":"6DqSAKvTET6g","executionInfo":{"status":"ok","timestamp":1706728660666,"user_tz":-60,"elapsed":432,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T07:38:48.673924Z","iopub.execute_input":"2024-02-01T07:38:48.674713Z","iopub.status.idle":"2024-02-01T07:38:49.051655Z","shell.execute_reply.started":"2024-02-01T07:38:48.674676Z","shell.execute_reply":"2024-02-01T07:38:49.050850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if GPU is available and move the model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel= nn.DataParallel(model)\nmodel.to(device)\n\n","metadata":{"id":"SrsuxKtgIPA9","executionInfo":{"status":"ok","timestamp":1706728661189,"user_tz":-60,"elapsed":526,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T07:38:52.147975Z","iopub.execute_input":"2024-02-01T07:38:52.148360Z","iopub.status.idle":"2024-02-01T07:38:52.204135Z","shell.execute_reply.started":"2024-02-01T07:38:52.148318Z","shell.execute_reply":"2024-02-01T07:38:52.202840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)\n","metadata":{"id":"seEjJV4YIQ8B","executionInfo":{"status":"ok","timestamp":1706728661190,"user_tz":-60,"elapsed":20,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"outputId":"719ad109-e49a-4cab-8cce-56589cbf756d","execution":{"iopub.status.busy":"2024-02-01T13:08:45.413092Z","iopub.execute_input":"2024-02-01T13:08:45.413786Z","iopub.status.idle":"2024-02-01T13:08:45.529552Z","shell.execute_reply.started":"2024-02-01T13:08:45.413757Z","shell.execute_reply":"2024-02-01T13:08:45.528355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Function and Optimizer:\n\n* Implement or use an existing Dice Loss function.\n* Choose an optimizer (e.g., Adam).","metadata":{"id":"TkUnhoDwLg43"}},{"cell_type":"code","source":"import torch\nfrom torch import Tensor\n\n\ndef dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n    # Average of Dice coefficient for all batches, or for a single mask\n    assert input.size() == target.size()\n    assert input.dim() == 3 or not reduce_batch_first\n\n    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n\n    inter = 2 * (input * target).sum(dim=sum_dim)\n    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n\n    dice = (inter + epsilon) / (sets_sum + epsilon)\n    return dice.mean()\n\n\ndef multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n    # Average of Dice coefficient for all classes\n    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)\n\n\ndef dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n    # Dice loss (objective to minimize) between 0 and 1\n    fn = multiclass_dice_coeff if multiclass else dice_coeff\n    return 1 - fn(input, target, reduce_batch_first=True)","metadata":{"id":"YsXOmft-wrM1","executionInfo":{"status":"ok","timestamp":1706728661190,"user_tz":-60,"elapsed":16,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T07:37:12.486567Z","iopub.execute_input":"2024-02-01T07:37:12.486970Z","iopub.status.idle":"2024-02-01T07:37:12.497146Z","shell.execute_reply.started":"2024-02-01T07:37:12.486940Z","shell.execute_reply":"2024-02-01T07:37:12.496405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"MYKkLpGawupH","executionInfo":{"status":"ok","timestamp":1706728661190,"user_tz":-60,"elapsed":15,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 1e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n#\ncriterion = nn.CrossEntropyLoss()\n","metadata":{"id":"vv09fvsQNvU0","executionInfo":{"status":"ok","timestamp":1706728661191,"user_tz":-60,"elapsed":14,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T07:37:12.498110Z","iopub.execute_input":"2024-02-01T07:37:12.498463Z","iopub.status.idle":"2024-02-01T07:37:12.512224Z","shell.execute_reply.started":"2024-02-01T07:37:12.498430Z","shell.execute_reply":"2024-02-01T07:37:12.511308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch import optim\nimport torch.utils.data as data\nimport torchvision.utils as utils\nfrom pathlib import Path\nimport tifffile\nimport numpy as np\n\n","metadata":{"id":"xAwN5vgd4--b","executionInfo":{"status":"ok","timestamp":1706728661191,"user_tz":-60,"elapsed":13,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T07:37:12.513258Z","iopub.execute_input":"2024-02-01T07:37:12.513520Z","iopub.status.idle":"2024-02-01T07:37:12.522670Z","shell.execute_reply.started":"2024-02-01T07:37:12.513498Z","shell.execute_reply":"2024-02-01T07:37:12.521888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef save_input_as_tiff(input_tensor, filename):\n    # Revert the normalization (assuming original range was 0-65535)\n    array = (input_tensor.detach().cpu().numpy() * 65535).astype(np.uint16)\n\n    # If the tensor is a 3D tensor (C, H, W), convert it to (H, W, C)\n    if array.ndim == 3:\n        array = np.transpose(array, (1, 2, 0))\n\n    # Save as TIFF in original format\n    tifffile.imwrite(filename, array)\n\ndef save_mask_as_tiff(mask_tensor, filename):\n    # Convert to NumPy array\n    array = mask_tensor.detach().cpu().numpy()\n\n    # No need to transpose masks as they should be [H, W]\n    tifffile.imwrite(filename, array)\n","metadata":{"id":"T3TYG7R1XqGE","executionInfo":{"status":"ok","timestamp":1706728661191,"user_tz":-60,"elapsed":13,"user":{"displayName":"Dan Benayoun","userId":"04048870769084533627"}},"execution":{"iopub.status.busy":"2024-02-01T07:37:12.523763Z","iopub.execute_input":"2024-02-01T07:37:12.524072Z","iopub.status.idle":"2024-02-01T07:37:12.533898Z","shell.execute_reply.started":"2024-02-01T07:37:12.524044Z","shell.execute_reply":"2024-02-01T07:37:12.533004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"      # Create a directory if it is not there, so we can save files and results in it\n      from pathlib import Path\n      Path('/kaggle/working/result/predicted').mkdir(parents=True, exist_ok=True)  \n      Path('/kaggle/working/result/original').mkdir(parents=True, exist_ok=True)\n      Path('/kaggle/working/result/label').mkdir(parents=True, exist_ok=True)\n    \n      Path('/kaggle/working/val/predicted').mkdir(parents=True, exist_ok=True)  \n      Path('/kaggle/working/val/original').mkdir(parents=True, exist_ok=True)\n      Path('/kaggle/working/val/label').mkdir(parents=True, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T07:37:12.535371Z","iopub.execute_input":"2024-02-01T07:37:12.535678Z","iopub.status.idle":"2024-02-01T07:37:12.547950Z","shell.execute_reply.started":"2024-02-01T07:37:12.535650Z","shell.execute_reply":"2024-02-01T07:37:12.547087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Initialize an empty DataFrame\nmetrics_data = pd.DataFrame(columns=['Epoch', 'Train Loss', 'Validation Loss', 'Validation Dice', 'Validation Sensitivity', 'Validation Specificity', 'Validation Precision', 'Validation IOU'])\n\n\nnum_epochs = 10  # Set the number of epochs\n\n\nfor epoch in range(num_epochs):\n  model.train()\n  total_train_loss = 0.0\n\n  # Set up tqdm progress bar\n  progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n  for batch_idx, (inputs, targets) in progress_bar:\n      inputs = inputs.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n      targets = targets.to(device=device, dtype=torch.long)\n\n      # Forward pass\n      outputs = model(inputs)\n\n      # Loss computation\n      loss = criterion(outputs, targets)\n      loss += dice_loss(\n            F.softmax(outputs, dim=1).float(),\n            F.one_hot(targets, model.module.out_dim).permute(0, 3, 1, 2).float(),\n            multiclass=True\n        )\n      # Backward and optimize\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      total_train_loss += loss.item()\n      progress_bar.set_postfix({\n            'loss': f'{total_train_loss / (batch_idx + 1):.4f}'\n        })\n      if batch_idx % 100 == 0 or batch_idx == len(data_loader)-1:\n          save_input_as_tiff(inputs[0], f\"/kaggle/working/result/original/original_image_{batch_idx}_{epoch}.tif\")\n          save_mask_as_tiff(targets[0], f\"/kaggle/working/result/label/label_image_{batch_idx}_{epoch}.tif\")\n          predicted_mask = torch.argmax(outputs[0],dim=0)\n          save_mask_as_tiff(predicted_mask,f\"/kaggle/working/result/predicted/predicted_image_{batch_idx}_{epoch}.tif\")\n          torch.save(model.module.state_dict(),Path(f'/kaggle/working/result/unet_{batch_idx}_{epoch}.pkl'))\n\n    # Validation Loop\n  sensitivities = []\n  specificities = []\n  precisions = []\n  ious = []        \n    \n  model.eval()\n  total_val_loss = 0.0\n  total_val_dice = 0\n  _ = 0\n  with torch.no_grad():\n      for inputs, targets in val_loader:\n          _ += 1\n          inputs, targets = inputs.to(device), targets.to(device)\n          outputs = model(inputs)\n          loss = criterion(outputs, targets)\n          total_val_loss += loss.item()\n                    # Convert outputs to probabilities\n          probabilities = torch.softmax(outputs, dim=1)\n\n          # Convert targets to one-hot format\n          target_one_hot = F.one_hot(targets, num_classes=model.module.out_dim).permute(0, 3, 1, 2).float()\n\n          # Calculate Dice coefficient\n          dice_score = multiclass_dice_coeff(probabilities, target_one_hot)\n          total_val_dice += dice_score\n          if _ % 100 == 0 or _ == len(val_loader) - 1:\n              save_input_as_tiff(inputs[0], f\"/kaggle/working/val/original/original_image_{batch_idx}_{epoch}.tif\")\n              save_mask_as_tiff(targets[0], f\"/kaggle/working/val/label/label_image_{batch_idx}_{epoch}.tif\")\n              predicted_mask = torch.argmax(outputs[0],dim=0)\n              save_mask_as_tiff(predicted_mask,f\"/kaggle/working/val/predicted/predicted_image_{batch_idx}_{epoch}.tif\")\n\n                  # Calculate additional metrics\n          preds = torch.argmax(outputs, dim=1)\n          for class_idx in range(model.module.out_dim):\n            true_positive = (preds == class_idx) & (targets == class_idx)\n            true_negative = (preds != class_idx) & (targets != class_idx)\n            false_positive = (preds == class_idx) & (targets != class_idx)\n            false_negative = (preds != class_idx) & (targets == class_idx)\n\n            TP = true_positive.sum().item()\n            TN = true_negative.sum().item()\n            FP = false_positive.sum().item()\n            FN = false_negative.sum().item()\n\n            sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n            specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n            precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n            iou = TP / (TP + FP + FN) if (TP + FP + FN) != 0 else 0\n\n            sensitivities.append(sensitivity)\n            specificities.append(specificity)\n            precisions.append(precision)\n            ious.append(iou)  \n\n  #Calculate average metrics\n  avg_train_loss = total_train_loss / len(data_loader)\n  avg_val_loss = total_val_loss / len(val_loader)\n  avg_val_dice = total_val_dice / len(val_loader)\n  avg_sensitivity = sum(sensitivities) / len(sensitivities)\n  avg_specificity = sum(specificities) / len(specificities)\n  avg_precision = sum(precisions) / len(precisions)\n  avg_iou = sum(ious) / len(ious)\n\n  # Append the current epoch's metrics to the DataFrame\n  current_epoch_data = {\n        'Epoch': epoch + 1,\n        'Train Loss': avg_train_loss,\n        'Validation Loss': avg_val_loss,\n        'Validation Dice': avg_val_dice,\n        'Validation Sensitivity': avg_sensitivity,\n        'Validation Specificity': avg_specificity,\n        'Validation Precision': avg_precision,\n        'Validation IOU': avg_iou\n    }\n  metrics_data = metrics_data.append(current_epoch_data, ignore_index=True)\n  # Optionally save to CSV after each epoch\n  metrics_data.to_csv('/kaggle/working/result/metrics_data.csv', index=False)\n","metadata":{"id":"WhklIV8xOsgE","outputId":"b1e73f7a-3578-4920-e8a5-46d5a5ddd164","execution":{"iopub.status.busy":"2024-02-01T07:55:59.017348Z","iopub.execute_input":"2024-02-01T07:55:59.017738Z","iopub.status.idle":"2024-02-01T07:57:21.273970Z","shell.execute_reply.started":"2024-02-01T07:55:59.017700Z","shell.execute_reply":"2024-02-01T07:57:21.272836Z"},"trusted":true},"execution_count":null,"outputs":[]}]}